{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Torch Dependencies \n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Importing OS Dependencies\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#Importing Other Dependencies\n",
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from termcolor import colored\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pre_Processing: Defining a Vocabulary class for building vocabulary from train Captions\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 100000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "def main_vocab():\n",
    "    caption_path='data/annotations/captions_train2014.json'\n",
    "    vocab_path='./data/vocab.pkl'\n",
    "    threshold=4\n",
    "    vocab = build_vocab(json=caption_path, threshold=threshold)\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "    print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pre-Processing: Resize an image to the 256x256.\n",
    "\n",
    "def resize_image(image, size):\n",
    "    return image.resize(size, Image.ANTIALIAS)\n",
    "\n",
    "def resize_images(image_dir, output_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = resize_image(img, size)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_images, output_dir))\n",
    "\n",
    "def main_resize():\n",
    "    image_dir = './data/train2014/'\n",
    "    output_dir = './data/resized2014/'\n",
    "    image_size = (256,256)\n",
    "    resize_images(image_dir, output_dir, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading :: COCO Custom Dataset compatible with torch.utils.data.DataLoader.\n",
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       json=json,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Model Architecture \n",
    "\n",
    "#Defining Encoder :: Load the pretrained ResNet-152 and replace top fc layer.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.autograd.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "#Defining Decoder :: Implementing Bi_directional Stacked LSTM with with Encoding and Multimodal layers\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size//2, num_layers, batch_first=True,bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size//2, num_layers, batch_first=True,bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens,_= self.lstm1(packed)\n",
    "        hiddens,_ = pad_packed_sequence(hiddens,batch_first=True)#hiddens.batch_sizes)\n",
    "        hiddens=hiddens[:, :, :512]\n",
    "        embeddings2 = torch.cat((features.unsqueeze(1), hiddens), 1)\n",
    "        packed2 = pack_padded_sequence(embeddings2, lengths, batch_first=True)\n",
    "        hiddens2,_= self.lstm2(packed2)\n",
    "        hiddens2=hiddens2[0][ :, :512] \n",
    "        outputs = self.linear(hiddens2)\n",
    "        return outputs\n",
    "    \n",
    "    def generate_caption(self, features, states_u=None,states_d=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs_u= features.unsqueeze(1)\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        vocab_path='data/vocab.pkl'\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            vocab = pickle.load(f)\n",
    "        temp=np.ones([1, 1], dtype=int)\n",
    "        ten=torch.from_numpy(temp)\n",
    "        tt=ten.to(device)\n",
    "        inputs_d=self.embed(tt)\n",
    "        \n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens_d, states_d = self.lstm1(inputs_d, states_d)\n",
    "            hiddens_u, states_u = self.lstm2(inputs_u, states_u)   # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens_u.squeeze(1))        # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                                # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs_d = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs_d = inputs_d.unsqueeze(1)\n",
    "            inputs_u= hiddens_d # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                 #sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "embed_size=512\n",
    "hidden_size=512\n",
    "num_layers=1\n",
    "log_step=10\n",
    "save_step=1000\n",
    "num_epochs=60\n",
    "batch_size=128\n",
    "num_workers=2\n",
    "learning_rate=0.001\n",
    "\n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "crop_size=224 \n",
    "transform = transforms.Compose([ \n",
    "transforms.RandomCrop(crop_size),\n",
    "transforms.RandomHorizontalFlip(), \n",
    "transforms.ToTensor(), \n",
    "transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                    (0.229, 0.224, 0.225))])\n",
    "    \n",
    "# Load vocabulary wrapper\n",
    "vocab_path='data/vocab.pkl'\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main_train():\n",
    "    \n",
    "    model_bi_path='models/'\n",
    "    image_dir='data/resized2014'\n",
    "    caption_path='data/annotations/captions_train2014.json'\n",
    "\n",
    "    # Create model directory\n",
    "    if not os.path.exists(model_bi_path):\n",
    "        os.makedirs(model_bi_path)\n",
    "    \n",
    "    # Build data loader\n",
    "    data_loader = get_loader(image_dir,caption_path, vocab, \n",
    "                             transform, batch_size,shuffle=True, num_workers=num_workers) \n",
    "    \n",
    "    # Build the models\n",
    "    encoder = Encoder(embed_size).to(device)\n",
    "    decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "    \n",
    "    # Train the models\n",
    "    total_step = len(data_loader_bi)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader_bi):            \n",
    "            # Set mini-batch dataset\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]        \n",
    "            # Forward, backward and optimize\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            outputs.requires_grad_(True)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print log info\n",
    "            if i % log_step == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "                \n",
    "            # Save the model checkpoints\n",
    "            if (i+1) % save_step == 0:\n",
    "                torch.save(decoder.state_dict(), os.path.join(\n",
    "                    model_bi_path, 'decoder-bi-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "                torch.save(encoder.state_dict(), os.path.join(\n",
    "                    model_bi_path, 'encoder-bi-{}-{}.ckpt'.format(epoch+1, i+1)))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "#post_processing\n",
    "def post_processing(sampled_ids):\n",
    "    temp=[]\n",
    "    for i in range(len(sampled_ids)-1):\n",
    "        if (sampled_ids[i]==sampled_ids[i+1]):\n",
    "            temp.append(i)\n",
    "    sampled_ids=np.delete(sampled_ids,temp)\n",
    "    temp=[]\n",
    "    for i in range(len(sampled_ids)-4):\n",
    "        if (sampled_ids[i],sampled_ids[i+1])==(sampled_ids[i+2],sampled_ids[i+3]):\n",
    "            temp.append(i)\n",
    "            temp.append(i+1)\n",
    "    sampled_ids=np.delete(sampled_ids,temp)\n",
    "    index_end = np.argwhere(sampled_ids==2)\n",
    "    sampled_ids = np.delete(sampled_ids,index_end)\n",
    "    index_unk = np.argwhere(sampled_ids==3)\n",
    "    sampled_ids = np.delete(sampled_ids,index_unk)\n",
    "    index_dot = np.argwhere(sampled_ids==19)\n",
    "    sampled_ids = np.delete(sampled_ids,index_dot)\n",
    "    index_com = np.argwhere(sampled_ids==87)\n",
    "    sampled_ids = np.delete(sampled_ids,index_com)\n",
    "    index_st= np.argwhere(sampled_ids==1)\n",
    "    sampled_ids = np.delete(sampled_ids,index_st)\n",
    "    \n",
    "    return sampled_ids\n",
    "\n",
    "def main_test():\n",
    "    \n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    files = [f for f in listdir('data/resizedval2014/') if isfile(join('data/resizedval2014/', f))]\n",
    "    \n",
    "    for i in range(5):#len(files)):\n",
    "        image='data/resizedval2014/'+files[i]\n",
    "        sh_image=image\n",
    "        encoder_bi_path='models/encoder-bi-2-2000.ckpt'\n",
    "        decoder_bi_path='models/decoder-bi-2-2000.ckpt'\n",
    "        \n",
    "    \n",
    "        # Model parameters (should be same as paramters in train.py)\n",
    "        # Build models\n",
    "        encoder = Encoder(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "        decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers)\n",
    "        encoder = encoder.to(device)\n",
    "        decoder = decoder.to(device)\n",
    "    \n",
    "       \n",
    "        # Load the trained model parameters\n",
    "        encoder.load_state_dict(torch.load(encoder_bi_path))\n",
    "        decoder.load_state_dict(torch.load(decoder_bi_path))\n",
    "        \n",
    "        # Prepare an image\n",
    "        image = load_image(image, transform)\n",
    "        image_tensor = image.to(device)\n",
    "        \n",
    "        # Generate an caption from the image\n",
    "        feature = encoder(image_tensor)\n",
    "        sampled_ids = decoder.generate_caption(feature)\n",
    "        sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "        sampled_ids = post_processing(sampled_ids)\n",
    "        # Convert word_ids to words\n",
    "        sampled_caption = []\n",
    "        for word_id in sampled_ids:\n",
    "            word = vocab.idx2word[word_id]\n",
    "            sampled_caption.append(word)\n",
    "            if word == '<end>':\n",
    "                break\n",
    "        sentence = ' '.join(sampled_caption)\n",
    "    \n",
    "         # Calculating Bleu score for forward and reverse caption \n",
    "        indices=ann2img[int(files[i][19:25])]\n",
    "        reference=[]\n",
    "        for ann_id in indices:\n",
    "            caption = coco.anns[ann_id]['caption']\n",
    "            reference.append(caption[:-1])\n",
    "        candidate=sentence\n",
    "        scores=nltk.translate.bleu_score.sentence_bleu(reference, candidate)\n",
    "        \n",
    "        # Print out the image and the generated caption\n",
    "        print(colored(('FORWARD -> '),'blue'),colored(sentence,'blue'),\n",
    "              colored((' -> '),'blue'),colored(scores,'green'))\n",
    "        I = io.imread(sh_image)\n",
    "        plt.imshow(I)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "#Mapping ann_id to img_id\n",
    "\n",
    "ann2img={}\n",
    "json='data/annotations/captions_val2014.json'\n",
    "coco = COCO(json)\n",
    "ids = list(coco.anns.keys())\n",
    "for index in range(0,len(ids)):\n",
    "    ann_id = ids[index]\n",
    "    img_id = coco.anns[ann_id]['image_id']\n",
    "    ann2img.setdefault(img_id, []).append(ann_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "# main() function calling which implicitly calls functions for pre-processing :: building vocabulary, resizing\n",
    "# Calls function for training and testing respectively\n",
    "if __name__ == '__main__':\n",
    "        print (\"================BUILDING VOCABULARY==============\")\n",
    "        #main_vocab()\n",
    "        print (\"=================RESIZING IMAGE==================\")\n",
    "        #main_resize()\n",
    "        print (\"=================MODEL TRAINING==================\")\n",
    "        #main_train()\n",
    "        print (\"==================MODEL TESTING==================\")\n",
    "        main_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre_processing to convert grey_scale to 3 channel conversion :: <Optional>\n",
    "\n",
    "'''\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "\n",
    "files = [f for f in listdir('data/resizedval2014/') if isfile(join('data/resizedval2014/', f))]\n",
    "for i in range(0,len(files)):\n",
    "    img =Image.open('data/resizedval2014/'+files[i])\n",
    "    nchannels=3\n",
    "    A= np.asarray(img)\n",
    "    if ((A.shape==(256,256))):\n",
    "        print(i)\n",
    "        stacked_img = np.stack((A,)*3, -1)\n",
    "        nimg = Image.fromarray(stacked_img, 'RGB')\n",
    "        nimg.save('data/resizedval2014/'+files[i])\n",
    "'''        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation :: Preparing Json File and calculating Bleu-4, METEOR, CIDEr score using COCO API\n",
    "'''data = []  \n",
    "vocab_path='data/out_cap.pkl'\n",
    "with open(vocab_path, 'rb') as g:\n",
    "    out_caption = pickle.load(g)\n",
    "\n",
    "files = [f for f in listdir('data/resizedval2014/') if isfile(join('data/resizedval2014/', f))]\n",
    "for i in range(0,len(files)):\n",
    "    candidate=out_caption[i][8:-6]   \n",
    "    data.append({'image_id': int(files[i][19:25]),'caption':candidate })\n",
    "with open('apiData.json', 'w') as outfile:  \n",
    "    json.dump(data, outfile)  \n",
    "    \n",
    "print(\"END\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
